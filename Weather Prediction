def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn import preprocessing
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
from sklearn.metrics import jaccard_score
from sklearn.metrics import f1_score
from sklearn.metrics import log_loss
from sklearn.metrics import confusion_matrix, accuracy_score
import sklearn.metrics as metrics
from sklearn.preprocessing import LabelEncoder

# Download Data 
path='https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillUp/labs/ML-FinalAssignment/Weather_Data.csv'
df = pd.read_csv(path)
df.head()

#Data Preprocessing
df_sydney_processed = pd.get_dummies(data= df, columns= ['RainToday', 'WindGustDir', 'WindDir9am', 'WindDir3pm'])
df_sydney_processed.drop('Date', axis=1, inplace = True)

#Data Model Training
features = df_sydney_processed.drop(columns= 'RainTomorrow', axis=1)
Y = df_sydney_processed['RainTomorrow']
x_train, x_test, y_train, y_test = train_test_split(features ,Y,test_size=0.2, random_state=10)

#Linear Regression 
#Create and train a linear regression model
LinearReg = linear_model.LinearRegression()
x = np.asanyarray(x_train)
y = np.asanyarray(y_train)
# Create a LabelEncoder object
le = LabelEncoder()

# Fit the encoder and transform from yes or no to 1 or 0
y = le.fit_transform(y_train)
LinearReg.fit(x, y)

#Prediction 
prediction = LinearReg.predict(x_test)

#Linear Regression Evaluation
yH = le.fit_transform(y_test)
LinearRegression_MAE = np.mean(np.absolute(prediction - yH))
LinearRegression_MSE = np.mean((prediction - yH)**2)
LinearRegression_R2 = metrics.r2_score(yH , prediction)
print(LinearRegression_MAE)
print(LinearRegression_MSE)
print(LinearRegression_R2)

#Reporting
metrics = { 'Metrics' : ["Mean Absolute Error (MAE)", "Mean Squared Error (MSE)", "R-squared (R2)"], 
            "value": [0.25631760994203784, 0.11572058282746588, 0.4271321073623009] }
Report = pd.DataFrame(metrics)
Report

#K-nearest neighbors (KNN) Data  Modelling
neigh = KNeighborsClassifier(n_neighbors = 4).fit(x_train, y_train)
predictions =neigh.predict(x_test)

#Calculate the value of the metric using predictions and the y_test dataframe
import sklearn.metrics as metrics
# Encode labels into numeric format
le = LabelEncoder()
# Encode 'No', 'Yes' to 0, 1 (or vice versa)
y_test_encoded = le.fit_transform(y_test)
# Ensure the predictions are encoded similarly
y_pred_encoded = le.transform(predictions)

# Calculate metrics
KNN_Accuracy_Score = metrics.accuracy_score(y_test_encoded, y_pred_encoded)
KNN_JaccardIndex = metrics.jaccard_score(y_test_encoded, y_pred_encoded)
KNN_F1_Score = metrics.f1_score(y_test_encoded, y_pred_encoded)

print(KNN_Accuracy_Score)
print(KNN_JaccardIndex)
print(KNN_F1_Score)

#Decision Tree Data modelling
Tree = DecisionTreeClassifier(criterion = "entropy", max_depth = 4)
Tree.fit(x_train,y_train)

#Prediction
predictions =  Tree.predict(x_test)

Tree_Accuracy_Score = accuracy_score(y_test, predictions)
Tree_JaccardIndex = jaccard_score(y_test, predictions, pos_label= 'No')
Tree_F1_Score = f1_score(y_test, predictions, pos_label= 'No')
print(Tree_Accuracy_Score)
print(Tree_JaccardIndex)
print(Tree_F1_Score)

#Logistic Regression
x_train, x_test, y_train, y_test = train_test_split(features, Y, test_size = 0.2, random_state =1) # Data Training
LR = LogisticRegression(C = 0.01, solver = 'liblinear').fit(x_train, y_train)
#Prediction 
prediction = LR.predict(x_test)
print(prediction)
predict_proba = LR.predict_proba(x_test)
print(predict_proba)
#Evaluation
LR_Accuracy_Score = accuracy_score(y_test, prediction)
LR_JaccardIndex = jaccard_score(y_test, prediction, pos_label= 'No')
LR_F1_Score = f1_score(y_test, prediction, pos_label= 'No')
LR_log_loss = log_loss(y_test,predict_proba)
print(LR_Accuracy_Score)
print(LR_JaccardIndex)
print(LR_F1_Score)
print(LR_log_loss)

#SVM Modelling
SVM = svm.SVC(kernel = 'linear')
SVM.fit(x_train, y_train)
predictions = SVM.predict(x_test)
SVM_Accuracy_Score = accuracy_score(y_test, predictions)
SVM_JaccardIndex = jaccard_score(y_test, predictions, pos_label= 'No')
SVM_F1_Score = f1_score(y_test, predictions, pos_label= 'No')
print(SVM_Accuracy_Score)
print(SVM_JaccardIndex)
print(SVM_F1_Score)

 #Reported accuracy of each classifier
 metrics={ 'Metrics': ['KNN_Accuracy_Score', 'KNN_JaccardIndex', 'KNN_F1_Score',
                       'Tree_Accuracy_Score', 'Tree_JaccardIndex', 'Tree_F1_Score',
                       'LR_Accuracy_Score', 'LR_JaccardIndex', 'LR_F1_Score', 'LR_log_loss',
                       'SVM_Accuracy_Score', 'SVM_JaccardIndex', 'SVM_F1_Score'],
           "value": [0.8183206106870229, 0.4251207729468599, 0.5966101694915255,
                     0.8183206106870229, 0.48034934497816595, 0.6489675516224189,
                     0.8274809160305343, 0.7941712204007286, 0.8852791878172589, 0.38008468000382284,
                     0.8458015267175573, 0.8126159554730983, 0.8966223132036848] }

Report = pd.DataFrame(metrics)
Report


